\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage, times}
\usepackage{ctable}
\begin{document}
\title{Deploying and Sharing QSAR Models using Predictive Modeling Markup Language}
\author{Villu Ruusmann${}^{\dagger}$ and Rajarshi Guha${}^{\ddagger}$\\
${}^{\ddagger}$National Center for Advancing Translational Sciences\\ 9800 Medical Center Drive  Rockville, MD 20850 \\ \\
${}^{\dagger}$Your address }
\date{}
\maketitle
\begin{abstract}
  A nice abstract
\end{abstract}

\section{A Plethora of Predictive Models}
\label{sec:introduction}

Quantitative Structure-Activity Relationship (QSAR) models have become
ubiquitous in computational chemistry and cheminformatics, having been
built for a variety of endpoints including biological activities and
physical properties. As shown in Figure \ref{fig:count-qsar} the last
15 years has seen a significant increase in publications developing or
using QSAR models. While there has been much discussion (REFS) on the
prospective utility and validity of predictive QSAR models, a key
bottleneck to their assessment is the fact that they are not easily
obtainable. That is, the bulk of QSAR models are described in a
publication in terms of the descriptors (sometimes made available) and
the modeling method. While the implementation is noted, it is upon the
reader to actually obtain the descriptor values, obtain and set up the
modeling environment and then actually rebuild the model. In other
words, the actual model that was developed by the authors is, usually,
not available to be used directly. In a few cases, the model may be
built in a pipeline tool [REF], which alleviates this problem to some
extent. Furthermore, the model development workflow usually involves
multiple software tools, each one of a specific version. Without
access to the exact same version of these tools, a user cannot be
guaranteed to exactly reproduce the model that the authors
describe. The issue of reproducibility (of workflows and results) has
become increasingly important [REF Greg Landrum] in the computational
arena.

A number of workers have described infrastructure to deploy predictive
models. Some platforms such as Pipeline Pilot provide a easy to use
method to deploy a complete pipeline as a web page. However, this
approach does not allow one to share models with users who do not have
access to the software. Guha [REF] described a web based
infrastructure that allowed one to deploy serialized models developed
in R. This architecture allowed one to distribute the model via a web
page, allowing users to provide new input and retrieve
predictions. But by virtue of using serialized R models, it allowed
the user to directly work with the original model itself. An important
downside was that the model itself did not contain explicit references
to the descriptors that were required as input. 

Bioclipse

\subsection{Serializing Models}
\label{sec:serializing-models}

A key step in sharing predictive models is serialization. That is,
converting the internal representation of the model developed in some
environment (R, Weka, Matlab, Mathematica etc) to a disk file (usually
a sequence of bytes). Not all deployment approaches require this step
- usually platforms that host the deployment and development
mechanisms (e.g., Pipeline Pilot, R using Shiny [REF]). But in general, for one to share a
model, it must be serialized and the serialized form is exchanged.

Most model development platforms support binary serialization
schemes. For example, R can save models in the RData format [REF]
which is binary and can only be read by the R platform (though since
it is Open Source, one could read/write such files using custom
code). XXX What about Matlab? XXX Java objects (say from Weka) can
be serialized using the Java Object Protocol [REF http://docs.oracle.com/javase/7/docs/technotes/guides/serialization/]

In contrast to binary serialization formats, the Predictive Modeling
Markup Language is a plain text XML format that is designed to fully
describe the model. Importantly, with appropriate software a PMML
document can be used to reconstruct the model, including any data
preprocessing steps (scaling, normalization) as well as generate input
descriptors if appropriately specified. A number of software tools are
capable of reading and writing PMML documents including SAS, SPSS,
TIBCO and KNIME, whereas others such as R can produce PMML documents
and WEKA which can consume PMML documents.


\ctable[cap={}, caption={Growth in number of Pubmed articles that use the term QSAR.},
label={fig:count-qsar},figure,botcap]
{c}
{}
{
  \includegraphics{img/count-qsar}
}

\section{Methods}
\label{sec:methods}

\subsection{Model Development}
\label{sec:model-development}

For this study we developed models in R 3.1 [REF] with descriptors
generated using the CDK [REF] via the rcdk [REF] package. As exemplars
we considered two datasets. XXX Describe the datasets.

Next we developed a linear regression (or classification) and random
forest  model  for each dataset. Using the pmml package [REF] we
serialized the models to PMML.

\subsection{Model Deployment}
\label{sec:model-deployment}


\section{Discussion}
\label{sec:discussion}

\subsection{PMML Supports Open Science}
\label{sec:pmml-supports-open}





\end{document}
